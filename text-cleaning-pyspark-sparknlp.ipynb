{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6y4FypgVNOCv","outputId":"bffe2483-5ca1-4161-9df0-e0155a320c3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zVkufvnyNTGd","outputId":"cb47dedd-7e70-45e9-cd10-cb090f030587"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting findspark\n","  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n","Installing collected packages: findspark\n","Successfully installed findspark-2.0.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[K     |████████████████████████████████| 281.4 MB 53 kB/s \n","\u001b[?25hCollecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[K     |████████████████████████████████| 199 kB 78.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845514 sha256=3d4c7f81a7cbd246eb16b8005c35c88b22ff7c240bdc5b68a5684576cd2ba142\n","  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sparknlp\n","  Downloading sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sparknlp) (1.21.6)\n","Collecting spark-nlp\n","  Downloading spark_nlp-4.2.3-py2.py3-none-any.whl (648 kB)\n","\u001b[K     |████████████████████████████████| 648 kB 33.0 MB/s \n","\u001b[?25hInstalling collected packages: spark-nlp, sparknlp\n","Successfully installed spark-nlp-4.2.3 sparknlp-1.0.0\n"]}],"source":["!pip install findspark\n","!pip install pyspark\n","!pip install sparknlp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3Vtq_zINUvC"},"outputs":[],"source":["from sparknlp.pretrained import PretrainedPipeline\n","import sparknlp\n","import pyspark.sql.functions as F\n","from sparknlp.base import *\n","from sparknlp.annotator import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XP8ijEMtPydr"},"outputs":[],"source":["spark = sparknlp.start(gpu=True)\n","sc = spark.sparkContext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpnIp4dsNan2"},"outputs":[],"source":["# comment = spark.read.parquet(\"/content/drive/MyDrive/data/comments_all\")\n","# submission = spark.read.parquet(\"/content/drive/MyDrive/data/submissions_all\")\n","# submission_all = submission.withColumn('text', F.concat(F.col('title'),F.lit(' '), F.col('selftext')))"]},{"cell_type":"markdown","metadata":{"id":"C1tT6XTROhpt"},"source":["# Dectect Languages for Submission. Only Keep English."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KItDsI26N2My"},"outputs":[],"source":["# submission_lang = language_detect_pipeline.transform(submission_all)\n","# submission_lang = submission_lang.withColumn(\"Russian\", submission_lang.language[0].metadata.ru)\n","# submission_lang = submission_lang.withColumn(\"Ukrainian\", submission_lang.language[0].metadata.uk)\n","# submission_lang = submission_lang.withColumn(\"English\", submission_lang.language[0].metadata.en)\n","# submission_lang = submission_lang.drop(\"language\", \"document\", \"sentence\")\n","# submission_en = submission_lang.filter(F.col('English')>0.8)\n","# submission_en.write.mode('overwrite').format(\"parquet\").save(\"/content/drive/MyDrive/data/submissions_en\")"]},{"cell_type":"markdown","metadata":{"id":"SB90BfYxOnbv"},"source":["# Dectect Languages for Comments. Only Keep English."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4RStAoqdObTC"},"outputs":[],"source":["# comment = comment.withColumnRenamed('body','text')\n","# comment_lang = language_detect_pipeline.transform(comment)\n","# comment_lang = comment_lang.withColumn(\"Russian\", comment_lang.language[0].metadata.ru)\n","# comment_lang = comment_lang.withColumn(\"Ukrainian\", comment_lang.language[0].metadata.uk)\n","# comment_lang = comment_lang.withColumn(\"English\", comment_lang.language[0].metadata.en)\n","# comment_lang = comment_lang.drop(\"language\", \"document\", \"sentence\")\n","# comment_en = comment_lang.filter(F.col('English')>0.8)\n","# comment_en.write.mode('overwrite').format(\"parquet\").save(\"/content/drive/MyDrive/data/comments_en\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CdxsgHrsOso2"},"outputs":[],"source":["comment_en = spark.read.parquet(\"/content/drive/Shareddrives/502_project/data/comments_en\")\n","submission_en = spark.read.parquet(\"/content/drive/Shareddrives/502_project/data/submissions_en\")"]},{"cell_type":"markdown","metadata":{"id":"O5eZubs7Zoug"},"source":["# Filter Empty Rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qug6t3noSOWw"},"outputs":[],"source":["remove = [\"\\[deleted\\]\",\"\\[removed\\]\", '\\[deleted by user\\]']\n","\n","submission_en = submission_en.withColumn('text', F.trim(F.regexp_replace('text','|'.join(['('+term+')' for term in remove]),'')))\n","submission_en = submission_en.filter(F.size(F.split(F.col('text'), ' '))>=2)\n","\n","comment_en = comment_en.withColumn('text', F.trim(F.regexp_replace('text','|'.join(['('+term+')' for term in remove]),'')))\n","comment_en = comment_en.filter(F.size(F.split(F.col('text'), ' '))>=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnjVh8tBPaov"},"outputs":[],"source":["documentAssembler = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","documentNormalizer = DocumentNormalizer() \\\n","    .setInputCols(\"document\") \\\n","    .setOutputCol(\"normalizedDocument\") \\\n","    .setAction(\"clean\") \\\n","    .setPatterns([\"[^A-Za-z]\"])\\\n","    .setReplacement(\" \") \\\n","    .setLowercase(True)\n","\n","tokenizer = Tokenizer() \\\n","  .setInputCols([\"normalizedDocument\"]) \\\n","  .setOutputCol(\"token\")\n","\n","stopwords_cleaner = StopWordsCleaner()\\\n","  .setInputCols(\"token\")\\\n","  .setOutputCol(\"cleanTokens\")\\\n","\n","tokenassembler = TokenAssembler()\\\n","    .setInputCols([\"normalizedDocument\", \"cleanTokens\"]) \\\n","    .setOutputCol(\"clean_text_tokens\")\n","\n","pipeline = Pipeline(stages=[documentAssembler,\n","                            documentNormalizer,\n","                            tokenizer, \n","                            stopwords_cleaner,\n","                            tokenassembler])\n","submission_cleaned = pipeline.fit(submission_en).transform(submission_en)\n","submission_cleaned = submission_cleaned.withColumn(\"cleaned_text\", F.explode(submission_cleaned.clean_text_tokens.result))\n","submission_cleaned = submission_cleaned.drop(\"document\", \"token\", \"cleanTokens\", \"clean_text_tokens\")\n","submission_cleaned.write.mode('overwrite').format(\"parquet\").save(\"/content/drive/Shareddrives/502_project/data/submissions_en_cleaned\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBTSSDjSaLoq"},"outputs":[],"source":["!wget -q https://raw.githubusercontent.com/******/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YazqfeVUp7A"},"outputs":[],"source":["documentAssembler = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","documentNormalizer = DocumentNormalizer() \\\n","    .setInputCols(\"document\") \\\n","    .setOutputCol(\"normalizedDocument\") \\\n","    .setAction(\"clean\") \\\n","    .setPatterns([\"[^A-Za-z]\"])\\\n","    .setReplacement(\" \") \\\n","    .setLowercase(True)\n","\n","tokenizer = Tokenizer() \\\n","  .setInputCols([\"normalizedDocument\"]) \\\n","  .setOutputCol(\"token\")\n","\n","stopwords_cleaner = StopWordsCleaner()\\\n","  .setInputCols(\"token\")\\\n","  .setOutputCol(\"cleanTokens\")\\\n","\n","stemmer = Stemmer() \\\n","    .setInputCols([\"cleanTokens\"]) \\\n","    .setOutputCol(\"stem\")\n","\n","lemmatizer = Lemmatizer() \\\n","    .setInputCols([\"stem\"]) \\\n","    .setOutputCol(\"lemma\") \\\n","    .setDictionary(\"./AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")\n","\n","tokenassembler = TokenAssembler()\\\n","    .setInputCols([\"normalizedDocument\", \"lemma\"]) \\\n","    .setOutputCol(\"clean_text_tokens\")\n","\n","pipeline = Pipeline(stages=[documentAssembler,\n","                            documentNormalizer,\n","                            tokenizer, \n","                            stopwords_cleaner,\n","                            stemmer,\n","                            lemmatizer,\n","                            tokenassembler])\n","\n","submission_cleaned = pipeline.fit(submission_en).transform(submission_en)\n","submission_cleaned = submission_cleaned.withColumn(\"cleaned_text\", F.explode(submission_cleaned.clean_text_tokens.result))\n","submission_cleaned = submission_cleaned.drop(\"document\", \"token\", \"cleanTokens\", \"clean_text_tokens\")\n","submission_cleaned.write.mode('overwrite').format(\"parquet\").save(\"/content/drive/Shareddrives/502_project/data/submissions_en_lemmatized\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mY3sZtqibJ5N"},"outputs":[],"source":["documentAssembler = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","documentNormalizer = DocumentNormalizer() \\\n","    .setInputCols(\"document\") \\\n","    .setOutputCol(\"normalizedDocument\") \\\n","    .setAction(\"clean\") \\\n","    .setPatterns([\"[^A-Za-z]\"])\\\n","    .setReplacement(\" \") \\\n","    .setLowercase(True)\n","\n","tokenizer = Tokenizer() \\\n","  .setInputCols([\"normalizedDocument\"]) \\\n","  .setOutputCol(\"token\")\n","\n","stopwords_cleaner = StopWordsCleaner()\\\n","  .setInputCols(\"token\")\\\n","  .setOutputCol(\"cleanTokens\")\\\n","\n","tokenassembler = TokenAssembler()\\\n","    .setInputCols([\"normalizedDocument\", \"cleanTokens\"]) \\\n","    .setOutputCol(\"clean_text_tokens\")\n","\n","pipeline = Pipeline(stages=[documentAssembler,\n","                            documentNormalizer,\n","                            tokenizer, \n","                            stopwords_cleaner,\n","                            tokenassembler])\n","comment_cleaned = pipeline.fit(comment_en).transform(comment_en)\n","comment_cleaned = comment_cleaned.withColumn(\"cleaned_text\", F.explode(comment_cleaned.clean_text_tokens.result))\n","comment_cleaned = comment_cleaned.drop(\"document\", \"token\", \"cleanTokens\", \"clean_text_tokens\")\n","comment_cleaned.write.mode('overwrite').format(\"parquet\").save(\"/content/drive/Shareddrives/502_project/data/comments_en_cleaned\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P0WpRy_QcZP-","outputId":"3ffbce5b-b1d3-45eb-f82c-84ff0698b1fc"},"outputs":[{"data":{"text/plain":["6762807"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["comment_en = spark.read.parquet(\"/content/drive/Shareddrives/502_project/data/comments_en_cleaned\")\n","comment_en.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ito9Ll0wliEB"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.0"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
